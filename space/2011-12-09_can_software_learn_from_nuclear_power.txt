Can software learn from nuclear power?

<img class="right shadow" src="mushroom_cloud.jpg" />

I just finished reading
[Charles Perrow](http://en.wikipedia.org/wiki/Charles_Perrow)'s
_[Normal Accidents](http://books.google.com/books?id=VC5hYoMw4N0C)_, the classic
book that examines why and how complex systems fail.

When it was published in 1984, the Internet and large distributed software
systems were still nascent, so he focused on nuclear power plants, chemical
plants, and marine and air traffic control. Even so, the parallels are
remarkable: buffers and queues, throttles and safety valves, abuse of common
subsystems, unexpected interactions, and incomplete and untrustworthy monitoring
abound. If you replace valves with network connections and gases with data, a
1970s power plant meltdown looks a lot like a 2011 cloud platform outage.

<!-- more -->
<p/>

The good news is, we've already learned some of Perrow's lessons. As he drills
into system architectures and accidents, he lays ground rules that we now know
well. Individual component failures are routine; handle them redundancy. Human
errors are inevitable; catch them with safeguards. Still, systems are complex
enough that unexpected interactions are common, especially during component
failures. This leads to large scale failures: "normal accidents" in his world,
outages in ours.

Continuing on familiar ground, Perrow builds a
[taxonomy of systems](http://books.google.com/books?id=VC5hYoMw4N0C&pg=PA97&lpg=PA97&ots=MCajdI32mf)
based on two criteria we know well: _complexity_ of interaction between parts
and how tight the _coupling_ is between connected parts. Connecting this to
failure rates, he concludes that systems with complex interactions and tight
coupling are by far the most failure prone. Sound familiar?

"Normal accident" rates are low for linear, loosely coupled systems. The Post
Office is a classic example. Interestingly, though, they're also low in linear,
tight systems such as assembly lines, as well as in complex, loose systems such
as universities. Both can fail, but the causes are straightforward. A system
must be both complex _and_ tightly coupled before it's prone to accidents. This
begs the question: can you prevent failures by making a system more linear or by
loosening its coupling? Perrow says yes.

<a class="shutter" href="perrow_interaction_coupling.png">
  <img class="right" src="perrow_interaction_coupling_thumb.png"></a>

Could we do the same with software we build? Loose coupling is still familiar
ground: we know that replacing an RPC call with a queue, for example, can
improve reliability. Linearizing is less obvious, though, especially since it
doesn't always mean simplifying. Can we make that online app look more like an
assembly line by separating data generation from serving? How about replacing
real time indexing with a batch indexer? Can we turn that big hairy
[MapReduce](http://en.wikipedia.org/wiki/MapReduce) into a pipeline?

Things get really interesting - and scary - when Perrow turns to
[look at operations](http://books.google.com/books?id=VC5hYoMw4N0C&lpg=PA97&ots=MCajdI32mf&pg=PA332).
He argues that tightly coupled systems have little room for error, so they're
best run by centralized organization. That works for linear systems too, since
they're generally well understood and can by run by rote, by-the-book processes.

Loosely coupled systems, on the other hand, have slack to absorb individual
errors, which allows for the out-of-the-box thinking common in decentralized
organizations. More importantly, complex systems _demand_ decentralization,
since unexpected interactions can often only be understood and handled by the
people closest to them.

This puts us between a rock and a hard place. Tightly coupled systems need
centralized ops; complex systems need decentralized. In the real world,
centralized often wins, but that just means complex failures often go unnoticed
or misdiagnosed for hours. If we can't rearchitect a complex, tightly coupled
system, are we doomed to run it poorly?

Many of the other conclusions apply too. We all know the common tension between
engineering and ops: ship fast and automate vs safety, reliability, and control.
[Continuous integration](http://en.wikipedia.org/wiki/Continuous_integration)
has begun to bridge the gap between speed and safety, but Perrow's data confirms
that there's no clear answer to automation vs. control. Automation improves
efficiency, throughput, and some failure recovery, but hurts visibility,
control, and flexibility on the part of human operators. Each has its own
Achilles heel, and neither prevents failures entirely.

<img class="right shadow" src="truck_accident.jpg" />

One particularly interesting point was externalities, especially production
pressure to minimize costs and downtime. Perrow turns
[risk homeostasis](http://en.wikipedia.org/wiki/Risk_homeostasis) on its head
with example after example of technological safeguards that operators abuse to
maintain the same accident rate while doing more, faster. Truckers who get
better brakes, for example, compensate by driving faster downhill, which lets
them do more jobs.

Similarly, when we firefight outages, the pressure to get the system back up can
be overwhelming. We don't have the time to fully investigate and understand what
happened, so we guess and take shortcuts and clean up later. Usually this is the
right choice, but not always.

Of course, there are clear differences between nuclear power plants and
software. We're not bound by most of the physical constraints on industrial
systems, which is both good and bad. We update our systems quickly and often,
sometimes making significant design changes. We have tools such as abstraction,
reuse, and automated testing that our industrial colleagues don't, or at least
not to the same degree.

Do these differences make us better off? Or do we just use them to squeeze more
complexity out of our systems, maintaining the reliability status quo? Would
designing more linearly improve our reliability, as Perrow shows it did with
some nuclear power plants? Have we learned any more about how to structure ops
organizations? I don't know, but I might have a better toolbox for handling
these kinds of questions in the future.

_Thanks to Mike Shields for recommending this book._
